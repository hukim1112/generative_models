{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFGAN Tutorial\n",
    "\n",
    "## Authors: Joel Shor, joel-shor\n",
    "\n",
    "### More complex examples, see [`tensorflow/models/gan`](https://github.com/tensorflow/models/tree/master/research/gan)\n",
    "\n",
    "\n",
    "This notebook will walk you through the basics of using [TFGAN](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gan) to define, train, and evaluate Generative Adversarial Networks. We describe the library's core features as well as some extra features. This colab assumes a familiarity with TensorFlow's Python API. For more on TensorFlow, please see [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n",
    "\n",
    "Please note that running on GPU will significantly speed up the training steps, but is not required.\n",
    "\n",
    "Last update: 2017-10-16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<a href='#installation_and_setup'>Installation and Setup</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#download_data'>Download Data</a><br>\n",
    "<a href='#unconditional_example'>Unconditional GAN example</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#unconditional_input'>Input pipeline</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#unconditional_model'>Model</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#unconditional_loss'>Loss</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#unconditional_train'>Train and evaluation</a><br>\n",
    "<a href='#ganestimator_example'>GANEstimator example</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#ganestimator_input'>Input pipeline</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#ganestimator_train'>Train</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#ganestimator_eval'>Eval</a><br>\n",
    "<a href='#conditional_example'>Conditional GAN example</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#conditional_input'>Input pipeline</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#conditional_model'>Model</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#conditional_loss'>Loss</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#conditional_train'>Train and evaluation</a><br>\n",
    "<a href='#infogan_example'>InfoGAN example</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#infogan_input'>Input pipeline</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#infogan_model'>Model</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#infogan_loss'>Loss</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#infogan_train'>Train and evaluation</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='installation_and_setup'></a>\n",
    "## Installation and setup\n",
    "\n",
    "To make sure that your version of TensorFlow has TFGAN included, run\n",
    "\n",
    "```\n",
    "python -c \"import tensorflow.contrib.gan as tfgan\"\n",
    "```\n",
    "\n",
    "You also need to install the TFGAN models library and the TF-Slim models library.\n",
    "\n",
    "To check that these two steps work, execute the [`Imports`](#imports) cell. If it complains about unknown modules, restart the notebook after moving to the TFGAN models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make TFGAN models and TF-Slim models discoverable.\n",
    "import sys\n",
    "import os\n",
    "# This is needed since the notebook is stored in the `tensorflow/models/gan` folder.\n",
    "sys.path.append('..')\n",
    "sys.path.append(os.path.join('..', 'slim'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Main TFGAN library.\n",
    "tfgan = tf.contrib.gan\n",
    "\n",
    "# TFGAN MNIST examples from `tensorflow/models`.\n",
    "from mnist import data_provider\n",
    "from mnist import util\n",
    "\n",
    "# TF-Slim data provider.\n",
    "from datasets import download_and_convert_mnist\n",
    "\n",
    "# Shortcuts for later.\n",
    "slim = tf.contrib.slim\n",
    "layers = tf.contrib.layers\n",
    "ds = tf.contrib.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common functions\n",
    "\n",
    "These functions are used by many examples, so we define them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
    "  \n",
    "\n",
    "def visualize_training_generator(train_step_num, start_time, data_np):\n",
    "    \"\"\"Visualize generator outputs during training.\n",
    "    \n",
    "    Args:\n",
    "        train_step_num: The training step number. A python integer.\n",
    "        start_time: Time when training started. The output of `time.time()`. A\n",
    "            python float.\n",
    "        data: Data to plot. A numpy array, most likely from an evaluated TensorFlow\n",
    "            tensor.\n",
    "    \"\"\"\n",
    "    print('Training step: %i' % train_step_num)\n",
    "    time_since_start = (time.time() - start_time) / 60.0\n",
    "    print('Time since start: %f m' % time_since_start)\n",
    "    print('Steps per min: %f' % (train_step_num / time_since_start))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.squeeze(data_np), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_digits(tensor_to_visualize):\n",
    "    \"\"\"Visualize an image once. Used to visualize generator before training.\n",
    "    \n",
    "    Args:\n",
    "        tensor_to_visualize: An image tensor to visualize. A python Tensor.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            images_np = sess.run(tensor_to_visualize)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.squeeze(images_np), cmap='gray')\n",
    "    return images_np\n",
    "\n",
    "def evaluate_tfgan_loss(gan_loss, name=None):\n",
    "    \"\"\"Evaluate GAN losses. Used to check that the graph is correct.\n",
    "    \n",
    "    Args:\n",
    "        gan_loss: A GANLoss tuple.\n",
    "        name: Optional. If present, append to debug output.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            gen_loss_np = sess.run(gan_loss.generator_loss)\n",
    "            dis_loss_np = sess.run(gan_loss.discriminator_loss)\n",
    "    if name:\n",
    "        print('%s generator loss: %f' % (name, gen_loss_np))\n",
    "        print('%s discriminator loss: %f'% (name, dis_loss_np))\n",
    "    else:\n",
    "        print('Generator loss: %f' % gen_loss_np)\n",
    "        print('Discriminator loss: %f'% dis_loss_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='download_data'></a>\n",
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_DATA_DIR = '/tmp/mnist-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files already exist. Exiting without re-creating them.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not tf.gfile.Exists(MNIST_DATA_DIR):\n",
    "    tf.gfile.MakeDirs(MNIST_DATA_DIR)\n",
    "\n",
    "download_and_convert_mnist.run(MNIST_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='unconditional_example'></a>\n",
    "# Unconditional GAN Example\n",
    "\n",
    "With unconditional GANs, we want a generator network to produce realistic-looking digits. During training, the generator tries to produce realistic-enough digits to 'fool' a discriminator network, while the discriminator tries to distinguish real digits from generated ones. See the paper ['NIPS 2016 Tutorial: Generative Adversarial Networks'](https://arxiv.org/pdf/1701.00160.pdf) by Goodfellow or ['Generative Adversarial Networks'](https://arxiv.org/abs/1406.2661) by Goodfellow et al. for more details.\n",
    "\n",
    "The steps to using TFGAN to set up an unconditional GAN, in the simplest case, are as follows:\n",
    "\n",
    "1. **Model**: Set up the generator and discriminator graphs with a [`GANModel`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L39) tuple. Use [`tfgan.gan_model`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L64) or create one manually.\n",
    "1. **Losses**: Set up the generator and discriminator losses with a [`GANLoss`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L115) tuple. Use [`tfgan.gan_loss`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L328) or create one manually.\n",
    "1. **Train ops**: Set up TensorFlow ops that compute the loss, calculate the gradients, and update the weights with a [`GANTrainOps`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L128) tuple. Use [`tfgan.gan_train_ops`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L476) or create one manually.\n",
    "1. **Run alternating train loop**: Run the training Ops. This can be done with [`tfgan.gan_train`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L661), or done manually.\n",
    "\n",
    "Each step can be performed by a TFGAN library call, or can be constructed manually for more control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='unconditional_input'></a>\n",
    "## Data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fceb177dcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our input pipeline. Pin it to the CPU so that the GPU can be reserved\n",
    "# for forward and backwards propogation.\n",
    "batch_size = 32\n",
    "with tf.device('/cpu:0'):\n",
    "    images, _, _ = data_provider.provide_data('train', batch_size, MNIST_DATA_DIR)\n",
    "\n",
    "#images.shape, TensorShape([Dimension(32), Dimension(28), Dimension(28), Dimension(1)])\n",
    "imgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\n",
    "# imgs_to_visualize.shape, TensorShape([Dimension(1), Dimension(56), Dimension(280), Dimension(1)])\n",
    "images = visualize_digits(imgs_to_visualize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='unconditional_model'></a>\n",
    "## Model\n",
    "\n",
    "Set up a [GANModel tuple](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L39), which defines everything we need to perform GAN training. You can create the tuple with the library functions, or you can manually create one.\n",
    "\n",
    "Define the GANModel tuple using the TFGAN library function.\n",
    "For the simplest case, we need the following:\n",
    "\n",
    "1. A generator function that takes input noise and outputs generated MNIST digits\n",
    "\n",
    "1. A discriminator function that takes images and outputs a probability of  being real or fake\n",
    "\n",
    "1. Real images\n",
    "\n",
    "1. A noise vector to pass to the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fn(noise, weight_decay=2.5e-5):\n",
    "    \"\"\"Simple generator to produce MNIST images.\n",
    "    \n",
    "    Args:\n",
    "        noise: A single Tensor representing noise.\n",
    "        weight_decay: The value of the l2 weight decay.\n",
    "    \n",
    "    Returns:\n",
    "        A generated image in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    with slim.arg_scope(\n",
    "        [layers.fully_connected, layers.conv2d_transpose],\n",
    "        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.fully_connected(noise, 1024)\n",
    "        net = layers.fully_connected(net, 7 * 7 * 256)\n",
    "        net = tf.reshape(net, [-1, 7, 7, 256])\n",
    "        net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d_transpose(net, 32, [4, 4], stride=2)\n",
    "        # Make sure that generator output is in the same range as `inputs`\n",
    "        # ie [-1, 1].\n",
    "        net = layers.conv2d(net, 1, 4, normalizer_fn=None, activation_fn=tf.tanh)\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_fn(img, unused_conditioning, weight_decay=2.5e-5):\n",
    "    \"\"\"Discriminator network on MNIST digits.\n",
    "    \n",
    "    Args:\n",
    "        img: Real or generated MNIST digits. Should be in the range [-1, 1].\n",
    "        unused_conditioning: The TFGAN API can help with conditional GANs, which\n",
    "            would require extra `condition` information to both the generator and the\n",
    "            discriminator. Since this example is not conditional, we do not use this\n",
    "            argument.\n",
    "        weight_decay: The L2 weight decay.\n",
    "    \n",
    "    Returns:\n",
    "        Logits for the probability that the image is real.\n",
    "    \"\"\"\n",
    "    with slim.arg_scope(\n",
    "        [layers.conv2d, layers.fully_connected],\n",
    "        activation_fn=leaky_relu, normalizer_fn=None,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay),\n",
    "        biases_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.conv2d(img, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d(net, 128, [4, 4], stride=2)\n",
    "        net = layers.flatten(net)\n",
    "        net = layers.fully_connected(net, 1024, normalizer_fn=layers.batch_norm)\n",
    "        return layers.linear(net, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gan_model in module tensorflow.contrib.gan.python.train:\n",
      "\n",
      "gan_model(generator_fn, discriminator_fn, real_data, generator_inputs, generator_scope='Generator', discriminator_scope='Discriminator', check_shapes=True)\n",
      "    Returns GAN model outputs and variables.\n",
      "    \n",
      "    Args:\n",
      "      generator_fn: A python lambda that takes `generator_inputs` as inputs and\n",
      "        returns the outputs of the GAN generator.\n",
      "      discriminator_fn: A python lambda that takes `real_data`/`generated data`\n",
      "        and `generator_inputs`. Outputs a Tensor in the range [-inf, inf].\n",
      "      real_data: A Tensor representing the real data.\n",
      "      generator_inputs: A Tensor or list of Tensors to the generator. In the\n",
      "        vanilla GAN case, this might be a single noise Tensor. In the conditional\n",
      "        GAN case, this might be the generator's conditioning.\n",
      "      generator_scope: Optional generator variable scope. Useful if you want to\n",
      "        reuse a subgraph that has already been created.\n",
      "      discriminator_scope: Optional discriminator variable scope. Useful if you\n",
      "        want to reuse a subgraph that has already been created.\n",
      "      check_shapes: If `True`, check that generator produces Tensors that are the\n",
      "        same shape as real data. Otherwise, skip this check.\n",
      "    \n",
      "    Returns:\n",
      "      A GANModel namedtuple.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If the generator outputs a Tensor that isn't the same shape as\n",
      "        `real_data`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tfgan.gan_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANModel Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trying to share variable Discriminator/fully_connected/weights, but specified shape (125440, 1024) and found shape (6272, 1024).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-72c7d94f244c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdiscriminator_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreal_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     generator_inputs=tf.random_normal([batch_size, noise_dims]))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Sanity check that generated images before training are garbage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/contrib/gan/python/train.py\u001b[0m in \u001b[0;36mgan_model\u001b[0;34m(generator_fn, discriminator_fn, real_data, generator_inputs, generator_scope, discriminator_scope, check_shapes)\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mdiscriminator_real_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcheck_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0cc27ca5cb32>\u001b[0m in \u001b[0;36mdiscriminator_fn\u001b[0;34m(img, unused_conditioning, weight_decay)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mfully_connected\u001b[0;34m(inputs, num_outputs, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[1;32m   1637\u001b[0m         \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m         _reuse=reuse)\n\u001b[0;32m-> 1639\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;31m# Add variables to collections.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \"\"\"\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    135\u001b[0m                                     \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                                     trainable=True)\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       self.bias = self.add_variable('bias',\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                    \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                    trainable=trainable and self.trainable)\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"constraint\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constraint\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mlayer_variable_getter\u001b[0;34m(getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1539\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_model_variable_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1540\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36m_model_variable_getter\u001b[0;34m(getter, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, rename, use_resource, **_)\u001b[0m\n\u001b[1;32m   1529\u001b[0m       \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m       \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m       custom_getter=getter, use_resource=use_resource)\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mmodel_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource)\u001b[0m\n\u001b[1;32m    260\u001b[0m                  \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                  \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                  use_resource=use_resource)\n\u001b[0m\u001b[1;32m    263\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource)\u001b[0m\n\u001b[1;32m    215\u001b[0m                   \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                   \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                   use_resource=use_resource)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf1.4p3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    745\u001b[0m         raise ValueError(\"Trying to share variable %s, but specified shape %s\"\n\u001b[1;32m    746\u001b[0m                          \" and found shape %s.\" % (name, shape,\n\u001b[0;32m--> 747\u001b[0;31m                                                    found_var.get_shape()))\n\u001b[0m\u001b[1;32m    748\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mdtype_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to share variable Discriminator/fully_connected/weights, but specified shape (125440, 1024) and found shape (6272, 1024)."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "noise_dims = 64\n",
    "gan_model = tfgan.gan_model(\n",
    "    generator_fn,\n",
    "    discriminator_fn,\n",
    "    real_data=images,\n",
    "    generator_inputs=tf.random_normal([batch_size, noise_dims]))\n",
    "\n",
    "# Sanity check that generated images before training are garbage.\n",
    "generated_data_to_visualize = tfgan.eval.image_reshaper(\n",
    "    gan_model.generated_data[:20,...], num_cols=10)\n",
    "visualize_digits(generated_data_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='unconditional_loss'></a>\n",
    "## Losses\n",
    "\n",
    "We next set up the GAN model losses.\n",
    "\n",
    "Loss functions are currently an active area of research. The [losses library](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/losses/python/losses_impl.py) provides some well-known or successful loss functions, such as the [original minimax](https://arxiv.org/abs/1406.2661), [Wasserstein](https://arxiv.org/abs/1701.07875) (by Arjovsky et al), and [improved Wasserstein](https://arxiv.org/abs/1704.00028) (by Gulrajani et al) losses. It is easy to add loss functions to the library as they are developed, and you can also pass in a custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the minimax loss from the original paper.\n",
    "vanilla_gan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    generator_loss_fn=tfgan.losses.minimax_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.minimax_discriminator_loss)\n",
    "\n",
    "# We can use the Wasserstein loss (https://arxiv.org/abs/1701.07875) with the \n",
    "# gradient penalty from the improved Wasserstein loss paper \n",
    "# (https://arxiv.org/abs/1704.00028).\n",
    "improved_wgan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    # We make the loss explicit for demonstration, even though the default is \n",
    "    # Wasserstein loss.\n",
    "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
    "    gradient_penalty_weight=1.0)\n",
    "\n",
    "# We can also define custom losses to use with the rest of the TFGAN framework.\n",
    "def silly_custom_generator_loss(gan_model, add_summaries=False):\n",
    "    return tf.reduce_mean(gan_model.discriminator_gen_outputs)\n",
    "def silly_custom_discriminator_loss(gan_model, add_summaries=False):\n",
    "    return (tf.reduce_mean(gan_model.discriminator_gen_outputs) -\n",
    "            tf.reduce_mean(gan_model.discriminator_real_outputs))\n",
    "custom_gan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    generator_loss_fn=silly_custom_generator_loss,\n",
    "    discriminator_loss_fn=silly_custom_discriminator_loss)\n",
    "\n",
    "# Sanity check that we can evaluate our losses.\n",
    "for gan_loss, name in [(vanilla_gan_loss, 'vanilla loss'), \n",
    "                       (improved_wgan_loss, 'improved wgan loss'), \n",
    "                       (custom_gan_loss, 'custom loss')]:\n",
    "    evaluate_tfgan_loss(gan_loss, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='unconditional_train'></a>\n",
    "## Training and Evaluation\n",
    "\n",
    "### Train Ops\n",
    "In order to train a GAN, we need to train both generator and discriminator networks using some variant of the alternating training paradigm. To do this, we construct a [GANTrainOps tuple](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L128) either manually or with a library call. We pass it the optimizers that we want to use, as well as any extra arguments that we'd like passed to slim's `create_train_op` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(0.001, beta1=0.5)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(0.0001, beta1=0.5)\n",
    "gan_train_ops = tfgan.gan_train_ops(\n",
    "    gan_model,\n",
    "    improved_wgan_loss,\n",
    "    generator_optimizer,\n",
    "    discriminator_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "TFGAN provides some standard methods of evaluating generative models. In this example, we use a pre-trained classifier to calculate what is called the 'Inception Score' from [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498) (by Salimans et al). This metric is a combined score of quality and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_to_eval = 500\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './mnist/data/classify_mnist_graph_def.pb'\n",
    "\n",
    "# For variables to load, use the same variable scope as in the train job.\n",
    "with tf.variable_scope('Generator', reuse=True):\n",
    "    eval_images = gan_model.generator_fn(\n",
    "        tf.random_normal([num_images_to_eval, noise_dims]))\n",
    "\n",
    "eval_score = util.mnist_score(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Steps\n",
    "\n",
    "Now we're ready to train. TFGAN handles the alternating training scheme that arises from the GAN minmax game. It also gives you the option of changing the ratio of discriminator updates to generator updates. Most applications (distributed setting, borg, etc) will use the [`gan_train`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L661) function, but we will use a different TFGAN utility and manually run the train ops so we can introspect more.\n",
    "\n",
    "This code block should take about **1 minute** to run on a GPU kernel, and about **5 minutes** on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the option to train the discriminator more than one step for every \n",
    "# step of the generator. In order to do this, we use a `GANTrainSteps` with \n",
    "# desired values. For this example, we use the default 1 generator train step \n",
    "# for every discriminator train step.\n",
    "train_step_fn = tfgan.get_sequential_train_steps()\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "loss_values, mnist_score_values  = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    with slim.queues.QueueRunners(sess):\n",
    "        start_time = time.time()\n",
    "        for i in range(801):\n",
    "            cur_loss, _ = train_step_fn(\n",
    "                sess, gan_train_ops, global_step, train_step_kwargs={})\n",
    "            loss_values.append((i, cur_loss))\n",
    "            if i % 100 == 0:\n",
    "                mnist_score_values.append((i, sess.run(eval_score)))\n",
    "            if i % 200 == 0:\n",
    "                print('Current loss: %f' % cur_loss)\n",
    "                print('Current MNIST score: %f' % mnist_score_values[-1][1])\n",
    "                visualize_training_generator(\n",
    "                    i, start_time, sess.run(generated_data_to_visualize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eval metrics over time.\n",
    "plt.title('MNIST Score per step')\n",
    "plt.plot(*zip(*mnist_score_values))\n",
    "plt.figure()\n",
    "plt.title('Training loss per step')\n",
    "plt.plot(*zip(*loss_values))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ganestimator_example'></a>\n",
    "# GANEstimator\n",
    "TensorFlow offers a tf.Estimators API that makes it easy to train models. TFGAN offers a tf.Estimators compatible `GANEstimator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ganestimator_input'></a>\n",
    "## Data input pipeline\n",
    "`tf.Estimators` use `input_fn` to pass data to the estimators. We need one data source for training and one for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def _get_train_input_fn(batch_size, noise_dims):\n",
    "    def train_input_fn():\n",
    "        with tf.device('/cpu:0'):\n",
    "            images, _, _ = data_provider.provide_data(\n",
    "                'train', batch_size, MNIST_DATA_DIR)\n",
    "        noise = tf.random_normal([batch_size, noise_dims])\n",
    "        return noise, images\n",
    "    return train_input_fn\n",
    "\n",
    "\n",
    "def _get_predict_input_fn(batch_size, noise_dims):\n",
    "    def predict_input_fn():\n",
    "        noise = tf.random_normal([batch_size, noise_dims])\n",
    "        return noise\n",
    "    return predict_input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ganestimator_train'></a>\n",
    "## Training\n",
    "Training with `tf.Estimators` is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NOISE_DIMS = 64\n",
    "NUM_STEPS = 2000\n",
    "\n",
    "# Initialize GANEstimator with options and hyperparameters.\n",
    "gan_estimator = tfgan.estimator.GANEstimator(\n",
    "    generator_fn=generator_fn,  # Same as before.\n",
    "    discriminator_fn=discriminator_fn,  # Same as before.\n",
    "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
    "    generator_optimizer=tf.train.AdamOptimizer(0.001, 0.5),\n",
    "    discriminator_optimizer=tf.train.AdamOptimizer(0.0001, 0.5),\n",
    "    add_summaries=tfgan.estimator.SummaryType.IMAGES)\n",
    "\n",
    "# Train estimator.\n",
    "train_input_fn = _get_train_input_fn(BATCH_SIZE, NOISE_DIMS)\n",
    "gan_estimator.train(train_input_fn, max_steps=NUM_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ganestimator_eval'></a>\n",
    "## Evaluation\n",
    "Visualize some sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference.\n",
    "predict_input_fn = _get_predict_input_fn(36, NOISE_DIMS)\n",
    "prediction_iterable = gan_estimator.predict(predict_input_fn)\n",
    "predictions = [prediction_iterable.next() for _ in range(36)]\n",
    "\n",
    "# Nicely tile output and visualize.\n",
    "image_rows = [np.concatenate(predictions[i:i+6], axis=0) for i in\n",
    "              range(0, 36, 6)]\n",
    "tiled_images = np.concatenate(image_rows, axis=1)\n",
    "\n",
    "# Visualize.\n",
    "plt.axis('off')\n",
    "plt.imshow(np.squeeze(tiled_images), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conditional_example'></a>\n",
    "# Conditional GAN Example\n",
    "\n",
    "In the conditional GAN setting on MNIST, we wish to train a generator to produce\n",
    "realistic-looking digits **of a particular type**. For example, we want to be able to produce as many '3's as we want without producing other digits. In contrast, in the unconditional case, we have no control over what digit the generator produces. In order to train a conditional generator, we pass the digit's identity to the generator and discriminator in addition to the noise vector. See [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) by Mirza and Osindero for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conditional_input'></a>\n",
    "\n",
    "## Data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our input pipeline. Pin it to the CPU so that the GPU can be reserved\n",
    "# for forward and backwards propogation.\n",
    "batch_size = 32\n",
    "with tf.device('/cpu:0'):\n",
    "    images, one_hot_labels, _ = data_provider.provide_data('train', batch_size, MNIST_DATA_DIR)\n",
    "\n",
    "# Sanity check that we're getting images.\n",
    "imgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\n",
    "visualize_digits(imgs_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conditional_model'></a>\n",
    "## Model\n",
    "\n",
    "We perform the same procedure as in the unconditional case, but pass the digit label to the generator and discriminator as well as a random noise vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_generator_fn(inputs, weight_decay=2.5e-5):\n",
    "    \"\"\"Generator to produce MNIST images.\n",
    "    \n",
    "    Args:\n",
    "        inputs: A 2-tuple of Tensors (noise, one_hot_labels).\n",
    "        weight_decay: The value of the l2 weight decay.\n",
    "\n",
    "    Returns:\n",
    "        A generated image in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    noise, one_hot_labels = inputs\n",
    "  \n",
    "    with slim.arg_scope(\n",
    "        [layers.fully_connected, layers.conv2d_transpose],\n",
    "        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.fully_connected(noise, 1024)\n",
    "        net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n",
    "        net = layers.fully_connected(net, 7 * 7 * 128)\n",
    "        net = tf.reshape(net, [-1, 7, 7, 128])\n",
    "        net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d_transpose(net, 32, [4, 4], stride=2)\n",
    "        # Make sure that generator output is in the same range as `inputs`\n",
    "        # ie [-1, 1].\n",
    "        net = layers.conv2d(net, 1, 4, normalizer_fn=None, activation_fn=tf.tanh)\n",
    "\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_discriminator_fn(img, conditioning, weight_decay=2.5e-5):\n",
    "    \"\"\"Conditional discriminator network on MNIST digits.\n",
    "    \n",
    "    Args:\n",
    "        img: Real or generated MNIST digits. Should be in the range [-1, 1].\n",
    "        conditioning: A 2-tuple of Tensors representing (noise, one_hot_labels).\n",
    "        weight_decay: The L2 weight decay.\n",
    "\n",
    "    Returns:\n",
    "        Logits for the probability that the image is real.\n",
    "    \"\"\"\n",
    "    _, one_hot_labels = conditioning\n",
    "    with slim.arg_scope(\n",
    "        [layers.conv2d, layers.fully_connected],\n",
    "        activation_fn=leaky_relu, normalizer_fn=None,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay),\n",
    "        biases_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.conv2d(img, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d(net, 128, [4, 4], stride=2)\n",
    "        net = layers.flatten(net)\n",
    "        net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n",
    "        net = layers.fully_connected(net, 1024, normalizer_fn=layers.batch_norm)\n",
    "        \n",
    "        return layers.linear(net, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANModel Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dims = 64\n",
    "conditional_gan_model = tfgan.gan_model(\n",
    "    generator_fn=conditional_generator_fn,\n",
    "    discriminator_fn=conditional_discriminator_fn,\n",
    "    real_data=images,\n",
    "    generator_inputs=(tf.random_normal([batch_size, noise_dims]), \n",
    "                      one_hot_labels))\n",
    "\n",
    "# Sanity check that currently generated images are garbage.\n",
    "cond_generated_data_to_visualize = tfgan.eval.image_reshaper(\n",
    "    conditional_gan_model.generated_data[:20,...], num_cols=10)\n",
    "visualize_digits(cond_generated_data_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conditional_loss'></a>\n",
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_loss = tfgan.gan_loss(\n",
    "    conditional_gan_model, gradient_penalty_weight=1.0)\n",
    "\n",
    "# Sanity check that we can evaluate our losses.\n",
    "evaluate_tfgan_loss(gan_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conditional_train'></a>\n",
    "## Training and Evaluation\n",
    "\n",
    "\n",
    "We use a slightly different learning rate schedule that involves decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(0.0009, beta1=0.5)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(0.00009, beta1=0.5)\n",
    "gan_train_ops = tfgan.gan_train_ops(\n",
    "    conditional_gan_model,\n",
    "    gan_loss,\n",
    "    generator_optimizer,\n",
    "    discriminator_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Since quantitative metrics for generators are sometimes tricky (see [A note on the evaluation of generative models](https://arxiv.org/abs/1511.01844) for some surprising ones), we also want to visualize our progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up class-conditional visualization. We feed class labels to the generator\n",
    "# so that the the first column is `0`, the second column is `1`, etc.\n",
    "images_to_eval = 500\n",
    "assert images_to_eval % 10 == 0\n",
    "\n",
    "random_noise = tf.random_normal([images_to_eval, 64])\n",
    "one_hot_labels = tf.one_hot(\n",
    "    [i for _ in xrange(images_to_eval // 10) for i in xrange(10)], depth=10) \n",
    "with tf.variable_scope(conditional_gan_model.generator_scope, reuse=True):\n",
    "    eval_images = conditional_gan_model.generator_fn(\n",
    "        (random_noise, one_hot_labels))\n",
    "reshaped_eval_imgs = tfgan.eval.image_reshaper(\n",
    "    eval_images[:20, ...], num_cols=10)\n",
    "\n",
    "# We will use a pretrained classifier to measure the progress of our generator. \n",
    "# Specifically, the cross-entropy loss between the generated image and the target \n",
    "# label will be the metric we track.\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './mnist/data/classify_mnist_graph_def.pb'\n",
    "xent_score = util.mnist_cross_entropy(\n",
    "    eval_images, one_hot_labels, MNIST_CLASSIFIER_FROZEN_GRAPH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Steps\n",
    "\n",
    "In this example, we train the generator and discriminator while keeping track of\n",
    "our important metric, the cross entropy loss with the real labels.\n",
    "\n",
    "This code block should take about **2 minutes** on GPU and **10 minutes** on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "train_step_fn = tfgan.get_sequential_train_steps()\n",
    "loss_values, xent_score_values  = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    with slim.queues.QueueRunners(sess):\n",
    "        start_time = time.time()\n",
    "        for i in xrange(2001):\n",
    "            cur_loss, _ = train_step_fn(\n",
    "                sess, gan_train_ops, global_step, train_step_kwargs={})\n",
    "            loss_values.append((i, cur_loss))\n",
    "            if i % 50 == 0:\n",
    "                xent_score_values.append((i, sess.run(xent_score)))\n",
    "            if i % 400 == 0:\n",
    "                print('Current loss: %f' % cur_loss)\n",
    "                print('Current cross entropy score: %f' % xent_score_values[-1][1])\n",
    "                visualize_training_generator(i, start_time, sess.run(reshaped_eval_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eval metrics over time.\n",
    "plt.title('Cross entropy score per step')\n",
    "plt.plot(*zip(*xent_score_values))\n",
    "plt.figure()\n",
    "plt.title('Training loss per step')\n",
    "plt.plot(*zip(*loss_values))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='infogan_example'></a>\n",
    "# InfoGAN Example\n",
    "\n",
    "InfoGAN is a technique to induce semantic meaning in the latent space of a GAN generator in an unsupervised way. In this example, the generator learns how to generate a specific digit **without ever seeing labels**. This is achieved by maximizing the mutual information between some subset of the noise vector and the generated images, while also trying to generate realistic images. See [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657) by Chen at al for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='infogan_input'></a>\n",
    "## Data input pipeline\n",
    "\n",
    "This is the same as the unconditional case (we don't need labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our input pipeline. Pin it to the CPU so that the GPU can be reserved\n",
    "# for forward and backwards propogation.\n",
    "batch_size = 32\n",
    "with tf.device('/cpu:0'):\n",
    "    images, _, _ = data_provider.provide_data('train', batch_size, MNIST_DATA_DIR)\n",
    "\n",
    "# Sanity check that we're getting images.\n",
    "imgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\n",
    "visualize_digits(imgs_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='infogan_model'></a>\n",
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infogan_generator(inputs, categorical_dim, weight_decay=2.5e-5):\n",
    "    \"\"\"InfoGAN discriminator network on MNIST digits.\n",
    "    \n",
    "    Based on a paper https://arxiv.org/abs/1606.03657 and their code\n",
    "    https://github.com/openai/InfoGAN.\n",
    "    \n",
    "    Args:\n",
    "        inputs: A 3-tuple of Tensors (unstructured_noise, categorical structured\n",
    "            noise, continuous structured noise). `inputs[0]` and `inputs[2]` must be\n",
    "            2D, and `inputs[1]` must be 1D. All must have the same first dimension.\n",
    "        categorical_dim: Dimensions of the incompressible categorical noise.\n",
    "        weight_decay: The value of the l2 weight decay.\n",
    "    \n",
    "    Returns:\n",
    "        A generated image in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    unstructured_noise, cat_noise, cont_noise = inputs\n",
    "    cat_noise_onehot = tf.one_hot(cat_noise, categorical_dim)\n",
    "    all_noise = tf.concat([unstructured_noise, cat_noise_onehot, cont_noise], axis=1)\n",
    "    \n",
    "    with slim.arg_scope(\n",
    "        [layers.fully_connected, layers.conv2d_transpose],\n",
    "        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.fully_connected(all_noise, 1024)\n",
    "        net = layers.fully_connected(net, 7 * 7 * 128)\n",
    "        net = tf.reshape(net, [-1, 7, 7, 128])\n",
    "        net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d_transpose(net, 32, [4, 4], stride=2)\n",
    "        # Make sure that generator output is in the same range as `inputs`\n",
    "        # ie [-1, 1].\n",
    "        net = layers.conv2d(net, 1, 4, normalizer_fn=None, activation_fn=tf.tanh)\n",
    "    \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infogan_discriminator(img, unused_conditioning, weight_decay=2.5e-5,\n",
    "                          categorical_dim=10, continuous_dim=2):\n",
    "    \"\"\"InfoGAN discriminator network on MNIST digits.\n",
    "    \n",
    "    Based on a paper https://arxiv.org/abs/1606.03657 and their code\n",
    "    https://github.com/openai/InfoGAN.\n",
    "    \n",
    "    Args:\n",
    "        img: Real or generated MNIST digits. Should be in the range [-1, 1].\n",
    "        unused_conditioning: The TFGAN API can help with conditional GANs, which\n",
    "            would require extra `condition` information to both the generator and the\n",
    "            discriminator. Since this example is not conditional, we do not use this\n",
    "            argument.\n",
    "        weight_decay: The L2 weight decay.\n",
    "        categorical_dim: Dimensions of the incompressible categorical noise.\n",
    "        continuous_dim: Dimensions of the incompressible continuous noise.\n",
    "    \n",
    "    Returns:\n",
    "        Logits for the probability that the image is real, and a list of posterior\n",
    "        distributions for each of the noise vectors.\n",
    "    \"\"\"\n",
    "    with slim.arg_scope(\n",
    "        [layers.conv2d, layers.fully_connected],\n",
    "        activation_fn=leaky_relu, normalizer_fn=None,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay),\n",
    "        biases_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.conv2d(img, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d(net, 128, [4, 4], stride=2)\n",
    "        net = layers.flatten(net)\n",
    "        net = layers.fully_connected(net, 1024, normalizer_fn=layers.layer_norm)\n",
    "    \n",
    "        logits_real = layers.fully_connected(net, 1, activation_fn=None)\n",
    "\n",
    "        # Recognition network for latent variables has an additional layer\n",
    "        encoder = layers.fully_connected(net, 128, normalizer_fn=layers.batch_norm)\n",
    "\n",
    "        # Compute logits for each category of categorical latent.\n",
    "        logits_cat = layers.fully_connected(\n",
    "            encoder, categorical_dim, activation_fn=None)\n",
    "        q_cat = ds.Categorical(logits_cat)\n",
    "\n",
    "        # Compute mean for Gaussian posterior of continuous latents.\n",
    "        mu_cont = layers.fully_connected(\n",
    "            encoder, continuous_dim, activation_fn=None)\n",
    "        sigma_cont = tf.ones_like(mu_cont)\n",
    "        q_cont = ds.Normal(loc=mu_cont, scale=sigma_cont)\n",
    "\n",
    "        return logits_real, [q_cat, q_cont]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InfoGANModel Tuple\n",
    "\n",
    "The InfoGAN model requires some extra information, so we use a subclassed tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of the structured and unstructured noise dimensions.\n",
    "cat_dim, cont_dim, noise_dims = 10, 2, 64\n",
    "\n",
    "generator_fn = functools.partial(infogan_generator, categorical_dim=cat_dim)\n",
    "discriminator_fn = functools.partial(\n",
    "    infogan_discriminator, categorical_dim=cat_dim,\n",
    "    continuous_dim=cont_dim)\n",
    "unstructured_inputs, structured_inputs = util.get_infogan_noise(\n",
    "    batch_size, cat_dim, cont_dim, noise_dims)\n",
    "\n",
    "infogan_model = tfgan.infogan_model(\n",
    "    generator_fn=generator_fn,\n",
    "    discriminator_fn=discriminator_fn,\n",
    "    real_data=images,\n",
    "    unstructured_generator_inputs=unstructured_inputs,\n",
    "    structured_generator_inputs=structured_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='infogan_loss'></a>\n",
    "## Losses\n",
    "\n",
    "The loss will be the same as before, with the addition of the mutual information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infogan_loss = tfgan.gan_loss(\n",
    "    infogan_model,\n",
    "    gradient_penalty_weight=1.0,\n",
    "    mutual_information_penalty_weight=1.0)\n",
    "\n",
    "# Sanity check that we can evaluate our losses.\n",
    "evaluate_tfgan_loss(infogan_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='infogan_train'></a>\n",
    "## Training and Evaluation\n",
    "\n",
    "This is also the same as in the unconditional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(0.001, beta1=0.5)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(0.00009, beta1=0.5)\n",
    "gan_train_ops = tfgan.gan_train_ops(\n",
    "    infogan_model,\n",
    "    infogan_loss,\n",
    "    generator_optimizer,\n",
    "    discriminator_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Generate some images to evaluate MNIST score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up images to evaluate MNIST score.\n",
    "images_to_eval = 500\n",
    "assert images_to_eval % cat_dim == 0\n",
    "\n",
    "unstructured_inputs = tf.random_normal([images_to_eval, noise_dims-cont_dim])\n",
    "cat_noise = tf.constant(range(cat_dim) * (images_to_eval // cat_dim))\n",
    "cont_noise = tf.random_uniform([images_to_eval, cont_dim], -1.0, 1.0)\n",
    "\n",
    "with tf.variable_scope(infogan_model.generator_scope, reuse=True):\n",
    "    eval_images = infogan_model.generator_fn(\n",
    "        (unstructured_inputs, cat_noise, cont_noise))\n",
    "\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './mnist/data/classify_mnist_graph_def.pb'\n",
    "eval_score = util.mnist_score(\n",
    "    eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)\n",
    "\n",
    "# Generate three sets of images to visualize the effect of each of the structured noise\n",
    "# variables on the output.\n",
    "rows = 2\n",
    "categorical_sample_points = np.arange(0, 10)\n",
    "continuous_sample_points = np.linspace(-1.0, 1.0, 10)\n",
    "noise_args = (rows, categorical_sample_points, continuous_sample_points,\n",
    "              noise_dims-cont_dim, cont_dim)\n",
    "\n",
    "display_noises = []\n",
    "display_noises.append(util.get_eval_noise_categorical(*noise_args))\n",
    "display_noises.append(util.get_eval_noise_continuous_dim1(*noise_args))\n",
    "display_noises.append(util.get_eval_noise_continuous_dim2(*noise_args))\n",
    "\n",
    "display_images = []\n",
    "for noise in display_noises:\n",
    "    with tf.variable_scope(infogan_model.generator_scope, reuse=True):\n",
    "        display_images.append(infogan_model.generator_fn(noise))\n",
    "\n",
    "display_img = tfgan.eval.image_reshaper(\n",
    "    tf.concat(display_images, 0), num_cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "train_step_fn = tfgan.get_sequential_train_steps()\n",
    "loss_values, mnist_score_values  = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    with slim.queues.QueueRunners(sess):\n",
    "        start_time = time.time()\n",
    "        for i in xrange(6001):\n",
    "            cur_loss, _ = train_step_fn(\n",
    "            sess, gan_train_ops, global_step, train_step_kwargs={})\n",
    "            loss_values.append((i, cur_loss))\n",
    "            if i % 100 == 0:\n",
    "                mnist_score_values.append((i, sess.run(eval_score)))\n",
    "            if i % 1500 == 0: \n",
    "                print('Current loss: %f' % cur_loss)\n",
    "                print('Current MNIST score: %f' % mnist_score_values[-1][1])\n",
    "                visualize_training_generator(i, start_time, sess.run(display_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eval metrics over time.\n",
    "plt.title('MNIST Score per step')\n",
    "plt.plot(*zip(*mnist_score_values))\n",
    "plt.figure()\n",
    "plt.title('Training loss per step')\n",
    "plt.plot(*zip(*loss_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip training and load from checkpoint\n",
    "\n",
    "Training a model to good results in a colab takes about 10 minutes. You can train a model below,\n",
    "but for now let's load a pretrained model and inspect the output.\n",
    "\n",
    "The first two rows show the effect of the categorical noise. The second two rows\n",
    "show the effect of the first continuous variable, and the last two rows show the effect\n",
    "of the second continuous variable. Note that the categorical variable controls\n",
    "the digit value, while the continuous variable controls line thickness and orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM variables are causing the checkpoint reload to choke, so omit them when \n",
    "# doing variable remapping.\n",
    "var_dict = {x.op.name: x for x in \n",
    "            tf.contrib.framework.get_variables('Generator/') \n",
    "            if 'Adam' not in x.name}\n",
    "tf.contrib.framework.init_from_checkpoint(\n",
    "    './mnist/data/infogan_model.ckpt',\n",
    "    var_dict)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    display_img_np = sess.run(display_img)\n",
    "plt.axis('off')\n",
    "plt.imshow(np.squeeze(display_img_np), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "tf1.4p3",
   "language": "python",
   "name": "tf1.4p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
